{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a0abe1",
   "metadata": {},
   "source": [
    "# Dave's Attempt at Mushroom Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c64512",
   "metadata": {},
   "source": [
    "## AWS Machine Learning Specialty Course 7.92\n",
    "\n",
    "\\[Pasted -v-\\]\n",
    "\n",
    "## Exercise - Mushroom Classification\n",
    "\n",
    "In this exercise, you need to classify mushroom as edible or poisonous.\n",
    "\n",
    "This data set is provided by UCI: https://archive.ics.uci.edu/ml/datasets/mushroom.  You can read the problem description and objective in the UCI website.\n",
    "\n",
    "Build a classifier using XGBoost. You also need to perform data cleanup and transformation before you can train on XGBoost.\n",
    "\n",
    "Complete Solution is available here (however, try to solve on your own):\n",
    "\n",
    "https://github.com/ChandraLingam/AmazonSageMakerCourse/tree/master/xgboost/MushroomClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48656cb",
   "metadata": {},
   "source": [
    "## Data Prep and Training in the same Notebook\n",
    "\n",
    "### Follow iris (and, for Q&R, the solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d66b8cd",
   "metadata": {},
   "source": [
    "### Mushroom Classification Dataset\n",
    "\n",
    "&nbsp;&nbsp;Input features, ready for a Python list:<br/>\n",
    "'cap-shape', 'cap-surface', 'cap-color', 'bruises','odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color','stalk-shape', 'stalk-root', 'stalk-surface-above-ring','stalk-surface-below-ring', 'stalk-color-above-ring','stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'\n",
    "\n",
    "<br/>\n",
    "\n",
    "&nbsp;&nbsp;Target:<br/>\n",
    "Is the mushroom edible?  True is edible, False is poisonous<br/>\n",
    "'mushroom_is_edible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "import requests       # Might need  `pip install requests`\n",
    "import zipfile as zf\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting the data\n",
    "data_url = \"https://archive.ics.uci.edu/static/public/73/mushroom.zip\"\n",
    "data_zip_filename = \"mushroom.zip\"\n",
    "data_filename = \"mushroom_all.csv\"\n",
    "\n",
    "# For zipfile\n",
    "working_dir = os.getcwd()\n",
    "unzipped_dir = \"mushroom_unzipped\"\n",
    "new_dirname = os.path.join(working_dir, unzipped_dir)\n",
    "pathlib.Path(new_dirname).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31525a5",
   "metadata": {},
   "source": [
    "Checked: The new directory is there. Hooray!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19aba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  new_dirname:\\n{new_dirname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get the zip - path and fact it's a zip from looking at \n",
    "#+ https://archive.ics.uci.edu/ml/datasets/mushroom\n",
    "mushroom_request = requests.get(data_url, \n",
    "                                allow_redirects=True)\n",
    "with open(data_zip_filename, 'wb') as fh:\n",
    "    fh.write(mushroom_request.content)\n",
    "##endof:  with open ... fh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36ce03",
   "metadata": {},
   "source": [
    "Checked: the new zip archive is there. Hooray again!\n",
    "\n",
    "Not sure about this next part, but I'm following Chandra's stuff as well as I know how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfile_thing = zf.ZipFile(\"mushroom.zip\")\n",
    "zipfile_thing.extractall(new_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551fa31",
   "metadata": {},
   "source": [
    "Checked: the contents of the zip are there. <strike>I've learned that `zipfile_thing.extractall(\".\")` isn't the way to go</strike> - that or I did something wrong on my local machine. Yeah, oops, I just tried it here on AWS, and it worked fine. It's nice to have it in another directory, though, especially since I want to change the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash is underneath this Notebook\n",
    "!ls -lah \"/home/ec2-user/SageMaker/AmazonSageMakerCourse/xgboost/dwb_Mushroom_Try_2023-07-29/mushroom_unzipped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!stat \"/home/ec2-user/SageMaker/AmazonSageMakerCourse/xgboost/dwb_Mushroom_Try_2023-07-29/mushroom_unzipped/expanded.Z\"\n",
    "print()\n",
    "print(\"I've done some more inspection on  expanded.Z ;\")\n",
    "print(\"it's an archive, but not a zip.\")\n",
    "print(\"Inside is a text file with longer names for characteristics.\")\n",
    "print(\"I'll skip it for now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the file we want.\n",
    "candidate_fname_1 = os.path.join(new_dirname, \n",
    "                                 \"agaricus-lepiota.data\")\n",
    "candidate_fname_2 = os.path.join(new_dirname, \n",
    "                                 \"agaricus-lepiota.names\")\n",
    "\n",
    "\n",
    "def head_filename(this_fname, n_lines_to_read=10):\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  First {n_lines_to_read} lines from\" + \n",
    "          f\"\\n{this_fname}\")\n",
    "    print(\"-\" * 5)\n",
    "    with open(this_fname, 'r', encoding=\"utf-8\") as fh1:\n",
    "        for i in range(n_lines_to_read):\n",
    "            # I won't use the i, but Q&R, whatever\n",
    "            print(fh1.readline())\n",
    "        ##endof:  for i in range(n_lines_to_read)\n",
    "    ##endof:  with open ... fh1\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "##endof:  head_filename(<params>)\n",
    "\n",
    "print()\n",
    "head_filename(candidate_fname_1)\n",
    "print()\n",
    "head_filename(candidate_fname_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98449d",
   "metadata": {},
   "source": [
    "We want `agaricus-lepiota.data`. That's not a huge surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(candidate_fname_1, data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e982aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking; might as well use that head_filename function\n",
    "head_filename(data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81880ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['mushroom_is_edible', 'cap-shape', 'cap-surface', 'cap-color', \n",
    "           'bruises','odor', 'gill-attachment', 'gill-spacing', 'gill-size', \n",
    "           'gill-color','stalk-shape', 'stalk-root', \n",
    "           'stalk-surface-above-ring','stalk-surface-below-ring', \n",
    "           'stalk-color-above-ring','stalk-color-below-ring', 'veil-type', \n",
    "           'veil-color', 'ring-number', 'ring-type', 'spore-print-color', \n",
    "           'population', 'habitat'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_filename, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00599b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b135863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check before encoding\n",
    "df['mushroom_is_edible'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['mushroom_is_edible'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee46a27",
   "metadata": {},
   "source": [
    "Now, we'll need to encode the letters as numbers\n",
    "```\n",
    "ref_enc = \"https://stackoverflow.com/questions/24458645/\" + \"\n",
    "\"label-encoding-across-multiple-columns-in-scikit-learn\"\n",
    "```\n",
    "\n",
    "Which reference was found in Chandra's stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d9fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(preprocessing.LabelEncoder)\n",
    "df = df.apply(lambda x: d[x.name].fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97959ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f202c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check after encoding\n",
    "df['mushroom_is_edible'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['mushroom_is_edible'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1bd5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nifty way to look at the data from Chandra\n",
    "for key in d.keys():\n",
    "    print(key, d[key].classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340705c3",
   "metadata": {},
   "source": [
    "And we see the nice question mark (`'?'`) in the stalk-root column that we read about from the dataset site. Those are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  What we, with xgboost, and SageMaker (with its xgboost)\n",
    "#+ will need, _without_ column names, plus target as \n",
    "#+ first column.\n",
    "#  It seems sklearn likes to have the column names (?)\n",
    "df.to_csv(\"mushroom_all_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c257e5",
   "metadata": {},
   "source": [
    "Let's follow `iris_data_preparation` for making our testing and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf40ee0",
   "metadata": {},
   "source": [
    "### Training and Validation Set\n",
    "\n",
    "### Target Variable as first column followed by input features:\n",
    "mushroom_is_edible, cap-shape, cap-surface, cap-color, bruises,odor, gill-attachment, gill-spacing, gill-size, gill-color,stalk-shape, stalk-root, stalk-surface-above-ring,stalk-surface-below-ring, stalk-color-above-ring,stalk-color-below-ring, veil-type, veil-color, ring-number, ring-type, spore-print-color, population, habitat\n",
    "\n",
    "### Training and Validation files do not have a column header\n",
    "\n",
    "#### (when feeding into sklearn or SageMaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eed8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Training  =  70% of the data \n",
    "#+                 (of already-separated training; \n",
    "#+                  nothing from test set)\n",
    "#  Validation = 30% of the data \n",
    "#+                 (of already-separating training;\n",
    "#+                  nothing from test set)\n",
    "#  We will randomize the order of the dataset entries\n",
    "\n",
    "subj_prefix = 'mushroom'\n",
    "\n",
    "fraction_for_training = 0.7\n",
    "rnd_seed = 5\n",
    "\n",
    "training_filename = subj_prefix + \"_train.csv\"\n",
    "validation_filename = subj_prefix + \"_validation.csv\"\n",
    "column_list_filename = subj_prefix + \"_train_column_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(rnd_seed)\n",
    "l_shuffle = list(df.index)\n",
    "np.random.shuffle(l_shuffle)\n",
    "df = df.iloc[l_shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers of entries (of rows) for each\n",
    "rows = df.shape[0]\n",
    "train = int(fraction_for_training * rows)\n",
    "test = rows - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b789c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92151fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Training Set\n",
    "df[:train].to_csv(training_filename,\n",
    "                  index=False, header=False,\n",
    "                  columns=columns\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Validation Set\n",
    "df[train:].to_csv(validation_filename,\n",
    "                  index=False, header=False,\n",
    "                  columns=columns\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Column List\n",
    "with open(column_list_filename, 'w') as f:\n",
    "    f.write(','.join(columns))\n",
    "##endof:  with open ... as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we have\n",
    "!ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef2382",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr/>\n",
    "<hr/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d91dc",
   "metadata": {},
   "source": [
    "## Train a model with Mushroom data using XGBoost algorithm\n",
    "\n",
    "### Doing the training in the same notebook as data prep\n",
    "\n",
    "Though the reading in of files shows that it could be done separately.\n",
    "\n",
    "### Model is trained with XGBoost, installed earlier in the notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list_file = \"mushroom_train_column_list.txt\"\n",
    "train_file = \"mushroom_train.csv\"\n",
    "validation_file = \"mushroom_validation.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \"\"\n",
    "with open(column_list_file, 'r') as tfh:\n",
    "    columns = tfh.read().split(',')\n",
    "##endof:  with open ... as tfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc177656",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386af067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0577e",
   "metadata": {},
   "source": [
    "Actually, what's below isn't necessary at all in the mushroom dataset\n",
    "\n",
    "\\# **&lt;not-needed-for-mushrooms&gt;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels=[0,1] # I'm almost positive this isn't necessary\n",
    "classes = ['e', 'p']\n",
    "le_2 = preprocessing.LabelEncoder()\n",
    "le_2.fit(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834df4e2",
   "metadata": {},
   "source": [
    "(From Chandra's notebook with the Iris dataset)\n",
    "\n",
    "**In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
    "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5d919",
   "metadata": {},
   "source": [
    "Doing all the tab_completions with le_2 I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_2.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0e6c1",
   "metadata": {},
   "source": [
    "\\# **&lt;/not-needed-for-mushrooms&gt;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column names as the file does not have a column header\n",
    "df_train =      pd.read_csv(train_file,      names=columns)\n",
    "df_validation = pd.read_csv(validation_file, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ad068",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_validation.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a180e36",
   "metadata": {},
   "source": [
    "#### Here, we'll actually split up the dataframes as needed and train with the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45555a",
   "metadata": {},
   "source": [
    "#### START: Optional inspection of how the dataframe gets split with iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c941fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.iloc[:,1:].head()) # All rows, columns 1 and on (zero-indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.iloc[:, 0].head()) #  All rows, column 0\n",
    "                                  #+ The column header\n",
    "                                  #+ is at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[:,0].ravel() # takes column zero and switches it to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac99d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.iloc[:,0].ravel()) # takes column zero and switches it to an array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913947c",
   "metadata": {},
   "source": [
    "#### ENDOF: Optional inspection of how the dataframe gets split with iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce93105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 1:]\n",
    "y_train = df_train.iloc[:,0].ravel()\n",
    "\n",
    "X_validation = df_validation.iloc[:, 1:]\n",
    "y_validation = df_validation.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815f64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a classifier\n",
    "# XGBoost Training Parameter Reference:\n",
    "#   https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "#classifier = xgb.XGBClassifier(objective='binary:logistic'\n",
    "#                               n_estimators=50)\n",
    "classifier = xgb.XGBClassifier(objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f378fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train,\n",
    "               y_train,\n",
    "               eval_set = [(X_train, y_train), (X_validation, y_validation)],\n",
    "               eval_metric=['logloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b21a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = classifier.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rounds = range(len(eval_result['validation_0']['logloss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=training_rounds,\n",
    "            y=eval_result['validation_0']['logloss'],\n",
    "            label=\"Training Error\")\n",
    "plt.scatter(x=training_rounds,\n",
    "            y=eval_result['validation_1']['logloss'],\n",
    "            label=\"Validation Error\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"LogLoss\")\n",
    "plt.title(\"Training Vs Validation Error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43322ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(validation_file, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed5c42",
   "metadata": {},
   "source": [
    "### Prediction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb28b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few of the predictions\n",
    "result[:5]  # shoot, all five are poisonous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the end look any better?\n",
    "result[-5:]  # all right, some edible ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424779fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_class'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ade90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e58673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f630e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mushroom_is_edible.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['mushroom_is_edible'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd5eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.predicted_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504993f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['predicted_class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8213ae",
   "metadata": {},
   "source": [
    "That looks uh pretty-pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ff67b",
   "metadata": {},
   "source": [
    "## Binary Classifier Metrics\n",
    "\n",
    "I'm just following the patterns from the solution, rather than an earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "# Explicitly stating labels. Pass=1, Fail=0\n",
    "def true_positive(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[1, 0])[0, 0]\n",
    "                         # positions in  confusion matrix -^--^-\n",
    "##endof:  true_positive(y_true, y_pred)\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[1, 0])[1, 1]\n",
    "##endof:  true_negative(y_true, y_pred)\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[1, 0])[1, 0]\n",
    "##endof:  false_positive(y_true, y_pred)\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[1, 0])[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39423bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Binary Classifier Metrics\n",
    "# Returns a dictionary {\"MetricName\":Value,...}\n",
    "\n",
    "def binary_classifier_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "\n",
    "    # References: \n",
    "    #  https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html\n",
    "    #  https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "    \n",
    "    # Definition:\n",
    "    # true positive = tp = how many samples were correctly classified as positive (count)\n",
    "    # true negative = tn = how many samples were correctly classified as negative (count)\n",
    "    # false positive = fp = how many negative samples were mis-classified as positive (count)\n",
    "    # false_negative = fn = how many positive samples were mis-classified as negative (count)\n",
    "    \n",
    "    # positive = number of positive samples (count)\n",
    "    #          = true positive + false negative\n",
    "    # negative = number of negative samples (count)\n",
    "    #          = true negative + false positive\n",
    "    \n",
    "    tp = true_positive(y_true, y_pred)\n",
    "    tn = true_negative(y_true, y_pred)\n",
    "    fp = false_positive(y_true, y_pred)\n",
    "    fn = false_negative(y_true, y_pred)\n",
    "    \n",
    "    positive = tp + fn\n",
    "    negative = tn + fp\n",
    "    \n",
    "    metrics['TruePositive'] = tp\n",
    "    metrics['TrueNegative'] = tn\n",
    "    metrics['FalsePositive'] = fp\n",
    "    metrics['FalseNegative'] = fn\n",
    "    \n",
    "    metrics['Positive'] = positive\n",
    "    metrics['Negative'] = negative\n",
    "    \n",
    "    # True Positive Rate (TPR, Recall) = true positive/positive\n",
    "    # How many positives were correctly classified? (fraction)\n",
    "    # Recall value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp/positive\n",
    "        \n",
    "    metrics['Recall'] = recall\n",
    "    \n",
    "    # True Negative Rate = True Negative/negative\n",
    "    # How many negatives were correctly classified? (fraction)\n",
    "    # True Negative Rate value closer to 1 is better. closer to 0 is worse\n",
    "    if tn == 0:\n",
    "        tnr = 0\n",
    "    else:\n",
    "        tnr = tn/(negative)\n",
    "    metrics['TrueNegativeRate'] = tnr\n",
    "    \n",
    "    # Precision = True Positive/(True Positive + False Positive)\n",
    "    # How many positives classified by the algorithm are really positives? (fraction)\n",
    "    # Precision value closer to 1 is better. closer to 0 is worse\n",
    "    if tp == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp/(tp + fp)\n",
    "    metrics['Precision'] = precision\n",
    "    \n",
    "    # Accuracy = (True Positive + True Negative)/(total positive + total negative)\n",
    "    # How many positives and negatives were correctly classified? (fraction)\n",
    "    # Accuracy value closer to 1 is better. closer to 0 is worse\n",
    "    accuracy = (tp + tn)/(positive + negative)\n",
    "    metrics['Accuracy'] = accuracy\n",
    "    \n",
    "    # False Positive Rate (FPR, False Alarm) = False Positive/(total negative)\n",
    "    # How many negatives were mis-classified as positives (fraction)\n",
    "    # False Positive Rate value closer to 0 is better. closer to 1 is worse\n",
    "    if fp == 0:\n",
    "        fpr = 0\n",
    "    else:\n",
    "        fpr = fp/(negative)\n",
    "    metrics['FalsePositiveRate'] = fpr\n",
    "    \n",
    "    # False Negative Rate (FNR, Misses) = False Negative/(total Positive)\n",
    "    # How many positives were mis-classified as negative (fraction)\n",
    "    # False Negative Rate value closer to 0 is better. closer to 1 is worse\n",
    "    fnr = fn/(positive)\n",
    "    metrics['FalseNegativeRate'] = fnr\n",
    "    \n",
    "    # F1 Score = harmonic mean of Precision and Recall\n",
    "    # F1 Score closer to 1 is better. Closer to 0 is worse.\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1 = 0\n",
    "    else:        \n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "    metrics['F1'] = f1\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803053ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "#{0:'edible',1:'poisonous'}) <-- WHAT!?!?!\n",
    "cnf_matrix = confusion_matrix(df['mushroom_is_edible'], \n",
    "                              df['predicted_class'],\n",
    "                              labels=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Poisonous\", \"Edible\"],\n",
    "                      title=\"Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix - fractions\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[\"Poisonous\", \"Edible\"],\n",
    "                      title=\"Confusion Matrix\",\n",
    "                      normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f72913",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [binary_classifier_metrics(df['mushroom_is_edible'], df['predicted_class'])]\n",
    "df_metrics=pd.DataFrame.from_dict(metrics)\n",
    "df_metrics.index = ['Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe81e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Counts')\n",
    "print(df_metrics[['TruePositive',\n",
    "                  'FalseNegative',\n",
    "                  'FalsePositive',\n",
    "                  'TrueNegative',]].round(2))\n",
    "print()\n",
    "print('Fractions')\n",
    "print(df_metrics[['Recall',\n",
    "                  'FalseNegativeRate',\n",
    "                  'FalsePositiveRate',\n",
    "                  'TrueNegativeRate',]].round(2))\n",
    "print()\n",
    "\n",
    "print(df_metrics[['Precision',\n",
    "                  'Accuracy',\n",
    "                  'F1']].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74757fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df['mushroom_is_edible'],\n",
    "                            df['predicted_class'],\n",
    "                            labels=[1, 0],\n",
    "                            target_names=['Poisonous','Edible']\n",
    "                           )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa5a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
