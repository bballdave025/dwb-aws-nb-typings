{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "782a07bf-08de-4030-88e1-6731c4ac956e"
    }
   },
   "source": [
    "## Train a model with Iris data using XGBoost algorithm\n",
    "###  Model is trained with XGBoost installed in notebook instance\n",
    "###  In the later examples, we will train using SageMaker's XGBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6c6a8672-d428-410a-82fa-7f587c9ef2ae"
    }
   },
   "outputs": [],
   "source": [
    "# Install xgboost in notebook instance.\n",
    "#### Command to install xgboost\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "652b58d4-3b75-405f-9f11-24d0cd1f9656"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a3946273-d086-4564-b0f1-6adc225191c3"
    }
   },
   "outputs": [],
   "source": [
    "column_list_file = 'iris_train_column_list.txt'\n",
    "train_file = 'iris_train.csv'\n",
    "validation_file = 'iris_validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7c803d6c-74cc-40d2-ab48-747ff4346c22"
    }
   },
   "outputs": [],
   "source": [
    "columns = ''\n",
    "with open(column_list_file,'r') as f:\n",
    "    columns = f.read().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "630dde8d-44b9-415d-8876-4e873407d0fc"
    }
   },
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Class Labels to integers\n",
    "# Labeled Classes\n",
    "labels=[0,1,2]\n",
    "classes = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d6ff2283-cb13-468f-b0cc-0aefeab7b57f"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the column names as the file does not have column header\n",
    "df_train = pd.read_csv(train_file,names=columns)\n",
    "df_validation = pd.read_csv(validation_file,names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a195ae30-1962-4427-859b-73a013dc10d6"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e30e8aeb-1ca2-4851-bc2d-1bdee29ab1cf"
    }
   },
   "outputs": [],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3b240613-803d-4fa9-93cf-53ef68df7b93"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:,1:] # Features: 1st column onwards \n",
    "y_train = df_train.iloc[:,0].ravel() # Target: 0th column\n",
    "\n",
    "X_validation = df_validation.iloc[:,1:]\n",
    "y_validation = df_validation.iloc[:,0].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9edc89e7-45d3-4350-9eb4-3e0938c3c55e"
    }
   },
   "outputs": [],
   "source": [
    "# Launch a classifier\n",
    "# XGBoost Training Parameter Reference: \n",
    "#   https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "classifier = xgb.XGBClassifier(objective=\"multi:softmax\",\n",
    "                               num_class=3,\n",
    "                               n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "348296fb-8c9b-4598-ad2e-d1fe8e10f76a"
    }
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train,\n",
    "               y_train,\n",
    "               eval_set = [(X_train, y_train), (X_validation, y_validation)],\n",
    "               eval_metric=['mlogloss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_before_early_stop = classifier.evals_result()\n",
    "training_rounds = range(len(eval_result_before_early_stop['validation_0']['mlogloss']))\n",
    "print(f\"training_rounds: {training_rounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=training_rounds,\n",
    "            y=eval_result_before_early_stop[\n",
    "                'validation_0']['mlogloss'],\n",
    "            label='Training Error')\n",
    "plt.scatter(x=training_rounds,\n",
    "            y=eval_result_before_early_stop[\n",
    "                'validation_1']['mlogloss'],label='Validation Error')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('LogLoss')\n",
    "plt.title('Training Vs Validation Error (Before early stop)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9839d7ce-e791-4d93-bc5f-28604ffde022"
    }
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train,\n",
    "               y_train,\n",
    "               eval_set = [(X_train, y_train), (X_validation, y_validation)],\n",
    "               eval_metric=['mlogloss'],\n",
    "               early_stopping_rounds=10)\n",
    "\n",
    "# early_stopping_rounds - needs to be passed in as a hyperparameter in SageMaker XGBoost implementation\n",
    "# \"The model trains until the validation score stops improving. \n",
    "# Validation error needs to decrease at least every early_stopping_rounds to continue training.\n",
    "# Amazon SageMaker hosting uses the best model for inference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e08f22c1-4346-4e2d-96a2-9974ed5c59ff"
    }
   },
   "outputs": [],
   "source": [
    "eval_result = classifier.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "092776c3-a611-4f40-91e2-664b3b99d05e"
    }
   },
   "outputs": [],
   "source": [
    "training_rounds = range(len(eval_result['validation_0']['mlogloss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2e9af3f7-fb85-4c52-83d5-ff9cae457294"
    }
   },
   "outputs": [],
   "source": [
    "print(training_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5e71239a-e321-43ba-ac2c-993b57b3be3a"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=training_rounds,y=eval_result['validation_0']['mlogloss'],label='Training Error')\n",
    "plt.scatter(x=training_rounds,y=eval_result['validation_1']['mlogloss'],label='Validation Error')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('LogLoss')\n",
    "plt.title('Training Vs Validation Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f144f315-6d38-429e-8c17-06c17a446198"
    }
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3312675d-307c-4eff-b835-34f0e7f57924"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(validation_file,names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "afad019f-88df-4893-bb3d-b7f2b7db214b"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9b5cb70d-6069-4511-810e-fd17e72667dd"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df.iloc[:,1:]\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f611c852-50e3-4a1a-9134-c1c6e82ad780"
    }
   },
   "outputs": [],
   "source": [
    "result = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2c573c2b-4143-4e01-b107-e6b871ce0249"
    }
   },
   "outputs": [],
   "source": [
    "df['predicted_class'] = result #le.inverse_transform(result)\n",
    "#DWB#                 = le.inverse_transform(result) #DWB# to get class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "5ad0fa04-6896-46b5-bc23-40d61480d7ca"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance of Actual and Model 1 Prediction\n",
    "plt.figure()\n",
    "plt.scatter(df.index,df['encoded_class'],label='Actual')\n",
    "plt.scatter(df.index,df['predicted_class'],label='Predicted',marker='^')\n",
    "plt.legend(loc=4)\n",
    "plt.yticks([0,1,2])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#  After I make a copy of this DataFrame that Chandra had\n",
    "#DWB#+ in the notebook\n",
    "df_orig = df.copy(deep=True)\n",
    "\n",
    "#DWB# Doing what was commented\n",
    "df['pred_cls_decoded'] = le.inverse_transform(result)\n",
    "df['decoded_cls'] = le.inverse_transform(df['encoded_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB# Seeing result of doing what was commented\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB# Getting the data back to normal for the confusion matrix\n",
    "df['pred_cls_enc'] = result\n",
    "df['enc_cls_chk'] = le.transform(df['decoded_cls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#  Checking it worked. \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#  I will actually get rid of those extra columns\n",
    "#DWB#+ before starting the next section.\n",
    "df.drop(columns=['pred_cls_decoded', 'decoded_cls', \n",
    "                 'pred_cls_enc', 'enc_cls_chk'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick, non-thorough check that we're back\n",
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>\n",
    "Confusion Matrix is a table that summarizes performance of classification model.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(df['encoded_class'],\n",
    "                              df['predicted_class'],labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                      title='Confusion matrix - Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=classes,\n",
    "                      title='Confusion matrix - Count',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df['encoded_class'],\n",
    "                            df['predicted_class'],\n",
    "                            labels=labels,\n",
    "                            target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
