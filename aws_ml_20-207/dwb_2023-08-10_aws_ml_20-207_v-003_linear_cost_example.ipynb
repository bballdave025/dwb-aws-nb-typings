{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Regression - Gradient Descent Overview</h4>\n",
    "<ul>\n",
    "<li>Linear Model. Estimated Target = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> \n",
    "+ w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub> \n",
    "+ â€¦ + w<sub>n</sub>x<sub>n</sub><br>\n",
    "where, w is the weight and x is the feature\n",
    "</li>\n",
    "<li>Predicted Value: Numeric</li>\n",
    "<li>Algorithm Used: Linear Regression. Objective is to find the weights w</li>\n",
    "<li>Optimization: Gradient Descent. Seeks to minimize loss/cost so that predicted value is as close to actual as possible</li>\n",
    "<li>Cost/Loss Calculation: Squared loss function</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Feature: x  \n",
    "\n",
    "Target: 5*x + 8 + some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Function\n",
    "def straight_line(x):\n",
    "    return 5*x + 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate predicted value for a given weight\n",
    "def predicted_at_weight(weight0, weight1, x):\n",
    "    return weight1*x + weight0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "samples = 150\n",
    "x = pd.Series(np.arange(0,150))\n",
    "y = x.map(straight_line) + np.random.randn(samples)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x':x,'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Feature example\n",
    "# Training Set - Contains several examples of feature 'x' and corresponding correct answer 'y'\n",
    "# Objective is to find out the form y = w0 + w1*x1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.x,df.y,label='Target')\n",
    "plt.grid(True)\n",
    "plt.xlim(-1,150)\n",
    "plt.ylim(0,800)\n",
    "plt.xlabel('Input Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(df[['x']],df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients:',reg.coef_,'Intercept:',reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Predict Y for different weights</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True function weight is  w1 = 5 and w0 = 8.  5*x + 8\n",
    "w0 = [10,3,10,15,100]\n",
    "w1 = [0,19,25,6,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = {}\n",
    "for i in range(len(w1)):\n",
    "    y_predicted['{0}-{1}'.format(w0[i],w1[i])] = predicted_at_weight(w0[i],w1[i], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,label='ground truth')\n",
    "\n",
    "for w in y_predicted.keys():\n",
    "    plt.plot(x,y_predicted[w],label=w)\n",
    "\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,700)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Predicted Output for different weights')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Squared Loss</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in y_predicted.keys():\n",
    "    squared_loss = (y-y_predicted[w])**2\n",
    "    print('Weight:{0}\\tLoss: {1:10.2f}'.format(w, squared_loss.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plot Loss at different weights for x</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a set of weights, let's find out loss or cost\n",
    "# True Function: 5x+8\n",
    "# Linear Regression algorithm iteratively tries to find the correct weight for x.\n",
    "# Let's test how the lost changes at different weights for x.\n",
    "\n",
    "# In this example, let's see how the \"loss\" changes for different weights\n",
    "#DWB# that is, for different values of w1 (with w0 held at the correct 8)\n",
    "weight = pd.Series(np.linspace(3,7,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight[:5])\n",
    "print()\n",
    "print(weight[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compute Loss using Squared Loss Function</h4>\n",
    "<h4>loss = average((true - predicted)^2)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost/Loss Calculation: Squared loss function...a measure of how far is predicted value from actual\n",
    "# Steps :\n",
    "\n",
    "#  For every weight for feature x, predict y\n",
    "#  Now, find out loss by = average ((actual - predicted)**2)\n",
    "\n",
    "#DWB# -v-Figuring out what's going on, here.\n",
    "do_see_the_guts = True\n",
    "this_count = 0\n",
    "this_count_max = 5\n",
    "\n",
    "#DWB# -v- Remember:\n",
    "# # Estimate predicted value for a given weight\n",
    "# def predicted_at_weight(weight0, weight1, x):\n",
    "#     return weight1*x + weight0\n",
    "\n",
    "loss_at_wt = []\n",
    "for w1 in weight:\n",
    "    y_predicted = predicted_at_weight(8,w1,x)\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        this_count += 1\n",
    "        if this_count == this_count_max:\n",
    "            break\n",
    "        ##endof:  if this_count == this_count_max\n",
    "        \n",
    "        print()\n",
    "        print(f\"  this_count:\\n{this_count}\")\n",
    "        print(f\"  this_count_max:\\n{this_count_max}\")\n",
    "        \n",
    "        print()\n",
    "        print(f\"  w1:\\n{w1}\")\n",
    "        print(f\"  type(w1):\\n{type(w1)}\")\n",
    "        print()\n",
    "        print(f\"  x:\\n{x}\")\n",
    "        print(f\"  type(x):\\n{type(x)}\")\n",
    "        print()\n",
    "        print(f\"  y_predicted = predicted_at_weight(8,{w1},x) =>\")\n",
    "        print(f\"  y_predicted =\\n{y_predicted}\")\n",
    "        print()\n",
    "        print(\"  Yay 1!\")\n",
    "        input(\"  Press enter to continue.\")\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    squared_error = (y - y_predicted)**2\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(f\"  y:\\n{y}\")\n",
    "        print(f\"  type(y):\\n{type(y)}\")\n",
    "        print()\n",
    "        print(f\"  y_predicted:\\n{y_predicted}\")\n",
    "        print(f\"  type(y_predicted):\\n{type(y_predicted)}\")\n",
    "        print()\n",
    "        print(f\"  squared_error:\\n{squared_error}\")\n",
    "        print(f\"  type(squared_error:\\n{type(squared_error)})\")\n",
    "        print()\n",
    "        print(\"  Yay 2!\")\n",
    "        input(\"  Press enter to continue.\")\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    # Average Squared Error at weight w1\n",
    "    loss_at_wt.append(squared_error.mean())\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(f\"  squared_error.mean():\\n{squared_error.mean()}\")\n",
    "        print(f\"  type(squared_error.mean()\\n{type(squared_error.mean())})\")\n",
    "        print()\n",
    "        print(f\"  loss_at_wt:\\n{loss_at_wt}\")\n",
    "        print(f\"  type(loss_at_wt):\\n{type(loss_at_wt)}\")\n",
    "        print()\n",
    "        print(\"  Yay 3!\")\n",
    "        input(\"  Press enter to continue.\")\n",
    "    ##endof:  if do_see_the_guts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#+ This doesn't really mean anything, since we\n",
    "#DWB#+ stopped after 4 <strike>or 5</strike> iterations.\n",
    "min(loss_at_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#  This one shouldn't even work, once again since\n",
    "#DWB#+ we stopped after the 4 <strike>5</strike> iterations. Let's see\n",
    "#DWB#+ the error, just for fun.\n",
    "\n",
    "#plt.scatter(x=weight, y=loss_at_wt)\n",
    "plt.plot(weight,loss_at_wt)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Weight for feature x')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve - Loss at different weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, yeah. I can run the loop through without the additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost/Loss Calculation: Squared loss function...a measure of how far is predicted value from actual\n",
    "# Steps :\n",
    "\n",
    "#  For every weight for feature x, predict y\n",
    "#  Now, find out loss by = average ((actual - predicted)**2)\n",
    "\n",
    "#DWB# -v-Figuring out what's going on, here.\n",
    "do_see_the_guts = False\n",
    "\n",
    "if do_see_the_guts:\n",
    "    this_count = 0\n",
    "    this_count_max = 5\n",
    "\n",
    "#DWB# -v- Remember:\n",
    "# # Estimate predicted value for a given weight\n",
    "# def predicted_at_weight(weight0, weight1, x):\n",
    "#     return weight1*x + weight0\n",
    "\n",
    "loss_at_wt = []\n",
    "for w1 in weight:\n",
    "    y_predicted = predicted_at_weight(8,w1,x)\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        this_count += 1\n",
    "        if this_count == this_count_max:\n",
    "            break\n",
    "        ##endof:  if this_count == this_count_max\n",
    "        \n",
    "        print()\n",
    "        print(f\"  this_count:\\n{this_count}\")\n",
    "        print(f\"  this_count_max:\\n{this_count_max}\")\n",
    "        \n",
    "        print()\n",
    "        print(f\"  w1:\\n{w1}\")\n",
    "        print(f\"  type(w1):\\n{type(w1)}\")\n",
    "        print()\n",
    "        print(f\"  x:\\n{x}\")\n",
    "        print(f\"  type(x):\\n{type(x)}\")\n",
    "        print()\n",
    "        print(f\"  y_predicted = predicted_at_weight(8,{w1},x) =>\")\n",
    "        print(f\"  y_predicted =\\n{y_predicted}\")\n",
    "        print()\n",
    "        print(\"  Yay 1!\")\n",
    "        input(\"  Press enter to continue.\")\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    squared_error = (y - y_predicted)**2\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(f\"  y:\\n{y}\")\n",
    "        print(f\"  type(y):\\n{type(y)}\")\n",
    "        print()\n",
    "        print(f\"  y_predicted:\\n{y_predicted}\")\n",
    "        print(f\"  type(y_predicted):\\n{type(y_predicted)}\")\n",
    "        print()\n",
    "        print(f\"  squared_error:\\n{squared_error}\")\n",
    "        print(f\"  type(squared_error:\\n{type(squared_error)})\")\n",
    "        print()\n",
    "        print(\"  Yay 2!\")\n",
    "        input(\"  Press enter to continue.\")\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    # Average Squared Error at weight w1\n",
    "    loss_at_wt.append(squared_error.mean())\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(f\"  squared_error.mean():\\n{squared_error.mean()}\")\n",
    "        print(f\"  type(squared_error.mean()\\n{type(squared_error.mean())})\")\n",
    "        print()\n",
    "        print(f\"  loss_at_wt:\\n{loss_at_wt}\")\n",
    "        print(f\"  type(loss_at_wt):\\n{type(loss_at_wt)}\")\n",
    "        print()\n",
    "        print(\"  Yay 3!\")\n",
    "        input(\"  Press enter to continue.\")\n",
    "    ##endof:  if do_see_the_guts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#  Since I ran the code a second time without the\n",
    "#DWB#+ additions (I didn't want to look up the pre-\n",
    "#DWB#+ additions code, so I just set the boolean for\n",
    "#DWB#+ running it to False the second time), this\n",
    "#DWB#+ cell and the next will mean something\n",
    "#min(loss_at_wt)\n",
    "\n",
    "#DWB#\n",
    "loss_to_find = min(loss_at_wt)\n",
    "print(loss_to_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(x=weight, y=loss_at_wt)\n",
    "plt.plot(weight,loss_at_wt)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Weight for feature x')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve - Loss at different weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#\n",
    "plt.plot(weight,loss_at_wt)\n",
    "plt.grid(True)\n",
    "plt.xlim(4.9, 5.1)\n",
    "plt.ylim(0, 200)\n",
    "plt.xlabel('Weight for feature x')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve - Loss at different weight; Zoomed to min')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DWB#  I guess this is very similar the original code, but I'm\n",
    "#DWB#+ adding details to find out more about the minimum.\n",
    "\n",
    "we_have_found_it = False\n",
    "\n",
    "for w1 in weight:\n",
    "    y_predicted = predicted_at_weight(8,w1,x)\n",
    "    squared_error = (y - y_predicted)**2\n",
    "    this_loss = squared_error.mean()\n",
    "    \n",
    "    if abs(this_loss - loss_to_find) < 0.1:\n",
    "        we_have_found_it = True\n",
    "        print( \"  We found it!\")\n",
    "        print(f\"  The minimum loss of: {this_loss}\")\n",
    "        print( \"  came at the (numerically-found) weight of: \" + \\\n",
    "              f\"{w1}\"\n",
    "             )\n",
    "        print(\"  (For 'numerically-found', you can read, 'approximate'.)\")\n",
    "    ##endof:  if abs(this_loss - loss_to_find) < 0.1\n",
    "    \n",
    "    if we_have_found_it:\n",
    "        break\n",
    "    \n",
    "##endof:  for w1 in weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Summary</h4>\n",
    "<h4>Squared Loss Function</h4>\n",
    "Squared Loss is the average of the squared difference between predicted and actual value.  This loss function not only gives us loss at a given weight; it also tells us which direction to go to minimize loss.<br>\n",
    "For a given weight, the algorithm finds the slope\n",
    "<ul>\n",
    "<li>If the slope is negative, then increase the weight</li>\n",
    "<li>If the slope is positive, then decrease the weight</li>\n",
    "</ul>\n",
    "\n",
    "<h4>Learning Rate</h4>\n",
    "Learning Rate parameter controls how much the weight should be increased or decreased<br>\n",
    "Too big of a change, the algorithm will skip the point where loss is minimal<br>\n",
    "Too small of a change, the algorithm will take several iterations to find the optimal weight<br>\n",
    "\n",
    "\n",
    "<h4>Gradient Descent</h4>\n",
    "Gradient Descent optimization computes the loss and slope, then adjusts the weights of all the features.<br>\n",
    "It iterates this process until it finds the optimal weight.<br>\n",
    "There are three flavors of Gradient descent:<br>\n",
    "\n",
    "<h4>Batch Gradient Descent</h4>\n",
    "Batch gradient descent computes loss for all examples in the training set and then adjusts the weight<br>\n",
    "It repeats this process for every iteration.<br>\n",
    "This process can be slow to converge when you have a large training data set<br>\n",
    "\n",
    "\n",
    "<h4>Stochastic Gradient Descent</h4>\n",
    "With Stochastic Gradient Descent, the algorithm computes loss for the next training example and immediately adjusts the weights.  This approach can help in converging to optimal weights for large data sets.<br>\n",
    "However, one problem with this approach is algorithm is adjusting weights based on a single example [our end objective is to find weight that works for all training examples and not for the immediate example], and this can result in wild fluctuation in weights.<br>\n",
    "\n",
    "\n",
    "<h4>Mini-Batch Gradient Descent</h4>\n",
    "Mini-batch Gradient descent combines benefit of Stochastic and Batch Gradient descent.<br>\n",
    "It adjusts the weight by testing few samples. The number of samples is defined by mini-batch size, typically around 128.<br>\n",
    "The mini-batch approach can be used to compute loss in parallel.<br>\n",
    "This technique is prevalent in deep learning and other algorithms.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
